# -*- coding: utf-8 -*-
"""hw2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oXAf8B58t4C7YXLybNCfTf4SeFVatv2u
"""

#!pip install python-louvain
import networkx as nx
#from community import community_louvain
import itertools
import json
import csv
import os
import datetime
import gzip


def get_name():
  return "Amit Sol"

def get_id():
  return 208254227

"""#Question 1

#i
"""

def community_detector(algorithm_name,network,most_valualble_edge=None):
    community_dict = {}
    #girvin_newman
    if algorithm_name == 'girvin_newman':
        modularity = float('-inf')
        k = int(nx.number_of_nodes(network))
        communities = nx.community.girvan_newman(network,most_valualble_edge)
        for i in itertools.islice(communities,k):
            curr_modularity = nx.community.modularity(network, i)
            if modularity < curr_modularity:
                modularity = curr_modularity
                communities_list = list(sorted(c) for c in next(communities))

        #num_partitions
        num_partitions = len(communities_list)
        #modularity
        modularity = nx.community.modularity(network, communities_list)
        #partition list
        partition = communities_list
  
  #louvain
    elif algorithm_name == 'louvain':
        communities = nx.community.louvain_communities(network)
        num_partitions = len(communities)
        #modularity
        modularity = nx.community.modularity(network, communities)
        #partition list
        partition = communities

  
    else:
        modularity = float('-inf')
        # find all maximal cliques in the graph
        cliques = list(nx.find_cliques(network))
        # find the largest clique
        largest_clique = len(max(cliques, key=len))
        for k in range(2,largest_clique+1):
            communities = list(nx.algorithms.community.k_clique_communities(network, k=k))
            communities_list =   [list(c) for c in communities]
        #compare every two lists and find their common elements
            for i in range(len(communities_list)):
                for j in range(i+1, len(communities_list)):
                    common_elements = set(communities_list[i]) & set(communities_list[j])
                    if len(common_elements) > 0:
                    # remove common elements from the larger list
                        if len(communities_list[i]) >= len(communities_list[j]):
                            communities_list[i] = list(set(communities_list[i]) - common_elements)
                        else:
                            communities_list[j] = list(set(communities_list[j]) - common_elements)
            comm2 = communities_list
            num_of_comm = len(communities_list)
            # find the nodes that are not present in any community
            community_nodes = set()
            for community in communities:
                community_nodes |= set(community)
            singletons = set(network.nodes()) - community_nodes
            # create a list of lists of singletons
            singleton_lists = [[node] for node in singletons]
            communities_list +=  singleton_lists
            modularity_curr = nx.community.modularity(network, communities_list)
            if modularity_curr > modularity:
                modularity = modularity_curr

                num_partitions = num_of_comm
                #partition list
                partition = comm2

  
    community_dict['num_partitions'] = num_partitions
    community_dict['modularity'] = modularity
    community_dict['partition'] = partition
    return community_dict

"""#ii"""

def edge_selector_optimizer(network):
  centrality = nx.edge_betweenness_centrality(network)
  return max(centrality, key=centrality.get)





"""#Question 2

#i
"""

def construct_heb_edges(files_path, start_date='2019-03-15', end_date='2019-04-15', non_parliamentarians_nodes=0):
    # excel file
    check_date = datetime.date(2022, 1, 1)
    start_date = datetime.datetime.strptime(start_date, "%Y-%m-%d").date()
    end_date = datetime.datetime.strptime(end_date, "%Y-%m-%d").date()
    folder_path = files_path
    if end_date < check_date:
        file_name = "central_political_players_2019.csv"
    else:
        file_name = "central_political_players_2022.csv"
    file_path = os.path.join(folder_path, file_name)

    # excel to dict
    with open(file_path, 'r') as csv_file:
        csv_reader = csv.reader(csv_file)
        my_dict = {rows[0]: rows[1] for rows in csv_reader}

    # Read the file contents into a string
    extra_nodes_dict = {}
    retweeted_count = {}
    retweet_dict = {}

    for file_name in os.listdir(folder_path):
        if file_name.endswith(".txt"):
            date_str = file_name[-14:-4]

        elif file_name.endswith(".gz"):
            date_str = file_name[-13:-3]
        else:
            continue
        date_of_interest = datetime.datetime.strptime(date_str, "%Y-%m-%d").date()
        if start_date <= date_of_interest <= end_date:
            file_path_txt = os.path.join(folder_path, file_name)
            if end_date < check_date:
                try:
                    with open(file_path_txt, 'r') as f:
                        # Read the contents of the file
                        contents = f.read()
                        dicts = contents.strip().split('\n')
                except EOFError:
                    continue
            else:
                file_path_gz = os.path.join(folder_path, file_name)
                try:
                    with gzip.open(file_path_gz, 'rt', encoding='utf-8') as f:
                        dicts = []
                        # contents = f.read()
                        for line in f:
                            dicts.append(line)

                except EOFError:
                    continue

                        # Split the file contents by newline character
            #dicts = contents.strip().split('\n')

                        # Parse each dictionary in the list of dictionaries
            for d in dicts:
                dictionary = json.loads(d)
                if "retweeted_status" in dictionary:
                    # Check if the value is present in the specified column
                    user_x = dictionary["user"]["id_str"]
                    user_y = dictionary["retweeted_status"]["user"]["id_str"]
                    if user_x in my_dict and user_y in my_dict:
                        if (user_x, user_y) not in retweet_dict:
                            retweet_dict[(user_x, user_y)] = 1
                        else:
                            retweet_dict[(user_x, user_y)] += 1

                    elif user_x in my_dict and user_y not in my_dict and non_parliamentarians_nodes != 0:
                        if user_y not in retweeted_count:
                            retweeted_count[user_y] = 1
                        else:
                            retweeted_count[user_y] += 1

                        if (user_x, user_y) not in extra_nodes_dict:
                            extra_nodes_dict[(user_x, user_y)] = 1
                        else:
                            extra_nodes_dict[(user_x, user_y)] += 1

    if non_parliamentarians_nodes > 0:

        most_common = [key for key, value in sorted(retweeted_count.items(), key=lambda item: item[1],reverse=True)]

        #print(retweeted_count)
        #sorted_pairs = sorted(extra_nodes_dict.items(), key=lambda x: x[1], reverse=True)
        #print(sorted_pairs)
        sorted_players = most_common[0:non_parliamentarians_nodes]

        for key,value in extra_nodes_dict.items():
            if key[1] in sorted_players:
                retweet_dict[key] = extra_nodes_dict[key]

            else:
                pass

    return retweet_dict

"""#iii"""

def construct_heb_network(edge_dictionary):
    G = nx.DiGraph()
    for edge, weight in edge_dictionary.items():
        G.add_edges_from([edge], weight=weight)
    return G

"""#iv"""


# if __name__ == "__main__":
#     from pyvis.network import Network
#     from IPython.display import display, HTML
#     #Question1
#     #iii
#
#     G = nx.les_miserables_graph()
#     print('girvan newman',community_detector('girvin_newman',G,edge_selector_optimizer))
#     print('louvain',community_detector('louvain',G))
#     print('clique_percolation',community_detector('clique_percolation',G))
#
#     def edge_selector_optimizer_improvment(network):
#         centrality = nx.edge_betweenness_centrality(network, weight="weight")
#         max_val = max(centrality.values())
#         for edge in centrality.keys():
#             centrality[edge] = centrality[edge] / max_val
#         return max(centrality, key=centrality.get)
#
#     print('girvan newman',community_detector('girvin_newman',G,edge_selector_optimizer_improvment))
#
#
#
#     def draw_graph(file_path, dict):
#         with open(file_path, 'r') as csv_file:
#             csv_reader = csv.reader(csv_file)
#             my_dict = {rows[0]: rows[1] for rows in csv_reader}
#
#         G = nx.Graph()
#
#         valid_colors = ['white', 'red', 'hotpink', 'aquamarine', 'azure', 'green', 'brown',
#                         'gray', 'blue', 'goldenrod', 'brown', 'burlywood', 'cadetblue', 'chartreuse',
#                         'chocolate', 'coral', 'cornflowerblue', 'cornsilk', 'crimson', 'cyan', 'darkblue', 'darkcyan',
#                         'darkgoldenrod', 'darkgray', 'darkgrey', 'darkgreen', 'darkkhaki', 'darkmagenta',
#                         'darkolivegreen',
#                         'darkorange', 'darkorchid', 'darkred', 'darksalmon', 'darkseagreen', 'darkslateblue',
#                         'darkslategray', 'darkslategrey', 'darkturquoise', 'darkviolet', 'deeppink', 'deepskyblue',
#                         'dimgray', 'dimgrey', 'dodgerblue', 'firebrick', 'floralwhite', 'forestgreen', 'fuchsia',
#                         'gainsboro', 'ghostwhite', 'gold', 'goldenrod', 'gray', 'grey', 'green', 'greenyellow',
#                         'honeydew',
#                         'hotpink', 'indianred', 'indigo', 'ivory', 'khaki', 'lavender', 'lavenderblush', 'lawngreen',
#                         'lemonchiff']
#
#         net = Network(width='100%', height='750px', bgcolor='#222222', font_color='white')
#         for edge, weight in dict.items():
#             if edge[0] in my_dict and edge[1] in my_dict:
#                 user_x, user_y = my_dict[edge[0]], my_dict[edge[1]]
#                 G.add_edge(user_x, user_y, weight=weight)
#             else:
#                 G.add_edge(my_dict[edge[0]], edge[1], weight=weight)
#
#         girvin_newman = community_detector('girvin_newman', G, edge_selector_optimizer)
#         print(girvin_newman)
#         communities = girvin_newman['partition']
#
#         for i, comm in enumerate(communities):
#             for node in comm:
#                 net.add_node(node, color=valid_colors[i])
#
#         for edge in G.edges():
#             net.add_edge(edge[0], edge[1])
#
#         html = net.generate_html()
#         with open("2019_test.html", mode='w', encoding='utf-8') as fp:
#             fp.write(html)
#         display(HTML(html))
#
#
#     dict1 = construct_heb_edges('C:/Users/Amit/Downloads/networks',start_date='2019-03-15', end_date='2019-04-15',non_parliamentarians_nodes=0)
#     dict2 = construct_heb_edges('C:/Users/Amit/Downloads/networks',start_date='2019-03-15', end_date='2019-04-15',non_parliamentarians_nodes=15)
#     print(construct_heb_network(dict2))
#     print('dict1',draw_graph('C:/Users/Amit/Downloads/networks/central_political_players_2019.csv',dict1))
#     print('dict2',draw_graph('C:/Users/Amit/Downloads/networks/central_political_players_2019.csv',dict2))
#
#     """#3
#
#     #ii + iii
#     """
#
#     dict3 = construct_heb_edges('C:/Users/Amit/Downloads/networks', start_date='2022-10-01', end_date='2022-10-31',non_parliamentarians_nodes=0)
#     dict4 = construct_heb_edges('C:/Users/Amit/Downloads/networks',start_date='2022-10-01',end_date='2022-10-31',non_parliamentarians_nodes=15)
#
#     print('dict3',draw_graph('C:/Users/Amit/Downloads/networks/central_political_players_2022.csv', dict3))
#     print('dict4', draw_graph('C:/Users/Amit/Downloads/networks/central_political_players_2022.csv', dict4))
#
